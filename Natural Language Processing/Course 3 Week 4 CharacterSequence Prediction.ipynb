{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Course 3 Week 4 CharacterSequence Prediction.ipynb","provenance":[],"authorship_tag":"ABX9TyMQps5jpeRtwu9Yrd6vxeHO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mY-Ab6D3zv8w"},"source":["## Natural Language Processing\n","\n","### Character Sequence Prediction on Shakespere Dataset"]},{"cell_type":"markdown","metadata":{"id":"z2o8zj_o0KaQ"},"source":["**Importing Library**"]},{"cell_type":"code","metadata":{"id":"iumd5COlzlwu","executionInfo":{"status":"ok","timestamp":1605804363941,"user_tz":-330,"elapsed":1196,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["import tensorflow as tf\n","\n","import numpy as np\n","import os\n","import time"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YavPDAep0sdQ"},"source":["Download the shakespeare dataset"]},{"cell_type":"code","metadata":{"id":"JTlJFmxl0qkh","executionInfo":{"status":"ok","timestamp":1605804366683,"user_tz":-330,"elapsed":1727,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YUFqhMr1crI"},"source":["### Reading the data\n","First, look in the text:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYOvuRZA1YlC","executionInfo":{"status":"ok","timestamp":1605804368510,"user_tz":-330,"elapsed":809,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"16c63de5-8c9b-4bb2-ad88-2ab25e24b70e"},"source":["# Read, then decode for py2 compat.\n","\n","text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n","\n","# length of text is the number of characters in it\n","\n","print('length of text: {} characters'.format(len(text)))"],"execution_count":42,"outputs":[{"output_type":"stream","text":["length of text: 1115394 characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hna1unJ95Q5i","executionInfo":{"status":"ok","timestamp":1605804370043,"user_tz":-330,"elapsed":913,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"797ace23-c84e-4a5b-c529-09492ce204e7"},"source":["# Take a look at the first 250 characters\n","\n","print(text[:250])"],"execution_count":43,"outputs":[{"output_type":"stream","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1labDAl5guP","executionInfo":{"status":"ok","timestamp":1605804370759,"user_tz":-330,"elapsed":1138,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"98135caa-7651-4d5c-bb48-004c71188a66"},"source":["# the unique characters in the file ( Vocab )\n","vocab = sorted(set(text))\n","print('{} unique characters..'.format(len(vocab)))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["65 unique characters..\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dAK9a6nu55d8"},"source":["### Process the Text\n","<br>\n","\n","**Vectorize the text**"]},{"cell_type":"code","metadata":{"id":"ra9vsoC153Le","executionInfo":{"status":"ok","timestamp":1605804373196,"user_tz":-330,"elapsed":1283,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["# creating a mapping from unique characters to indices\n","\n","char2idx = {u:i for i,u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","text_to_int = np.array([char2idx[c] for c in text])"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZ-_vvXD7Mev","executionInfo":{"status":"ok","timestamp":1605804375683,"user_tz":-330,"elapsed":1820,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"c05ce140-9b3c-4458-a5be-80a3bdf77c7e"},"source":["print('{')\n","for char,_ in zip(char2idx, range(20)):\n","  print('     {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n","print('  ....\\n')"],"execution_count":46,"outputs":[{"output_type":"stream","text":["{\n","     '\\n':   0,\n","     ' ' :   1,\n","     '!' :   2,\n","     '$' :   3,\n","     '&' :   4,\n","     \"'\" :   5,\n","     ',' :   6,\n","     '-' :   7,\n","     '.' :   8,\n","     '3' :   9,\n","     ':' :  10,\n","     ';' :  11,\n","     '?' :  12,\n","     'A' :  13,\n","     'B' :  14,\n","     'C' :  15,\n","     'D' :  16,\n","     'E' :  17,\n","     'F' :  18,\n","     'G' :  19,\n","  ....\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uldJ-RLH8Eix","executionInfo":{"status":"ok","timestamp":1605804377884,"user_tz":-330,"elapsed":1558,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"4975dac6-93cf-4a99-eeb9-98d5d48f602c"},"source":["print('{} --------------> characters mapped to int -------------> {}'.format(repr(text[:13]), text_to_int[:13]))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["'First Citizen' --------------> characters mapped to int -------------> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jIa58qTb95xp"},"source":["## The prediction task:"]},{"cell_type":"markdown","metadata":{"id":"lE813gfXBOvq"},"source":["Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n","\n","For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n","\n","So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\"."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcPN1-0Q8gR5","executionInfo":{"status":"ok","timestamp":1605804380795,"user_tz":-330,"elapsed":1843,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"7fad4463-cc63-46a9-a841-0ec8e15d36f4"},"source":["# the maximum length sentence you want for a single input in characters\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)\n","\n","# creating training examples/ targets \n","char_dataset = tf.data.Dataset.from_tensor_slices(text_to_int)\n","\n","for i in char_dataset.take(5):\n","  print(idx2char[i.numpy()])"],"execution_count":48,"outputs":[{"output_type":"stream","text":["F\n","i\n","r\n","s\n","t\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FMLKIdGVBR7Z"},"source":["The `batch` method lets us easily convert these individual characters to sequences of the desired size."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFjMGKSzAzhr","executionInfo":{"status":"ok","timestamp":1605804382845,"user_tz":-330,"elapsed":939,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"1e6e5675-8716-4318-8390-54866aba6850"},"source":["sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))"],"execution_count":49,"outputs":[{"output_type":"stream","text":["'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fHTm6lSrCV3R","executionInfo":{"status":"ok","timestamp":1605804385068,"user_tz":-330,"elapsed":1320,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["def split_input_target(chunk):\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","  return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3KJ9pZYvEcwR"},"source":["Print the first example input and target values:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uVswc4SEaK1","executionInfo":{"status":"ok","timestamp":1605804388112,"user_tz":-330,"elapsed":1512,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"a400b170-c3ab-4a21-c625-e91d0c911683"},"source":["for input_example, target_example in dataset.take(1):\n","  print('input data: ', repr(''.join(idx2char[input_example.numpy()])))\n","  print('target data: ', repr(''.join(idx2char[target_example.numpy()])))"],"execution_count":51,"outputs":[{"output_type":"stream","text":["input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","target data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mWPUDHbjF2e8"},"source":["Each index of these vectors is processed as a one time step. For the input at time step 0, the model receives the index for \"F\" and tries to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ns9uyPOOFqx8","executionInfo":{"status":"ok","timestamp":1605804392995,"user_tz":-330,"elapsed":1411,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"4a0d7997-863c-465c-e6c9-b0f3f758410c"},"source":["for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","  print(\"Step {:4d}\".format(i))\n","  print(\"Input: {}  ({})\".format(input_idx, repr(idx2char[input_idx])))\n","  print(\"Target: {}  ({})\".format(target_idx, repr(idx2char[target_idx])))\n","  print()"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Step    0\n","Input: 18  ('F')\n","Target: 47  ('i')\n","\n","Step    1\n","Input: 47  ('i')\n","Target: 56  ('r')\n","\n","Step    2\n","Input: 56  ('r')\n","Target: 57  ('s')\n","\n","Step    3\n","Input: 57  ('s')\n","Target: 58  ('t')\n","\n","Step    4\n","Input: 58  ('t')\n","Target: 1  (' ')\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6mLEc4YhHIAr"},"source":["### Creating Training Batch\n","\n","You used `tf.data` to split the text into managable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcFiJ04NIVxN","executionInfo":{"status":"ok","timestamp":1605804395787,"user_tz":-330,"elapsed":1481,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"c0de6734-f554-4717-d477-fd7b152904c5"},"source":["len(dataset)"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11043"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHaKNTQmG8M7","executionInfo":{"status":"ok","timestamp":1605804398032,"user_tz":-330,"elapsed":1348,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"73a850cc-1d98-4444-9d3c-2e75bb515bd0"},"source":["# Batch size\n","BATCH_SIZE = 64\n","\n","# buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences, So it doesnt attempt to shuffle the entire sequence in memory. Instead,\n","#  it maintains a buffer in which it shuffles elements).\n","\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n","\n","dataset"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"3nhh2SusLaZP"},"source":["## Build The Model"]},{"cell_type":"markdown","metadata":{"id":"6zfD00XKLg24"},"source":["Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n","\n","* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n","* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n","* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."]},{"cell_type":"code","metadata":{"id":"UiLIbAZiIdkc","executionInfo":{"status":"ok","timestamp":1605804406502,"user_tz":-330,"elapsed":1482,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["vocab_size = len(vocab)\n","\n","# the embedding dimensions\n","embedding_dim = 256\n","\n","# number of RNN units\n","rnn_units = 1024"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYsd6EPGRr_x","executionInfo":{"status":"ok","timestamp":1605804408523,"user_tz":-330,"elapsed":1754,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["def build_model(vocab_size, embedding_dim,rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","                               tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n","                               tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful = True, recurrent_initializer='glorot_uniform'),\n","                               tf.keras.layers.Dense(vocab_size)\n","  ])\n","  return model"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ioz7EGrXStyP","executionInfo":{"status":"ok","timestamp":1605804410242,"user_tz":-330,"elapsed":1383,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"a12865df-e2b0-4008-9a56-e8304a94f8cb"},"source":["model = build_model(\n","    vocab_size, embedding_dim, rnn_units, BATCH_SIZE\n",")"],"execution_count":57,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nFxf1sVd3FEf"},"source":["### Trying the model\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bITJCBiaTMaD","executionInfo":{"status":"ok","timestamp":1605795519524,"user_tz":-330,"elapsed":7485,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"d6a394c6-3e05-4e2a-9062-5b791dc10c88"},"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","  example_batch_predictions = model(input_example_batch)\n","  print(example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lKOaFgys32gE"},"source":["> **NOTE** : In the above example the sequence length of the input is  100 but the model can be run on inputs of any length."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3oPzrf53v5i","executionInfo":{"status":"ok","timestamp":1605796085875,"user_tz":-330,"elapsed":1556,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"1ed59aa1-5364-4b88-d87b-be66564bfcaf"},"source":["model.summary()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (64, None, 256)           16640     \n","_________________________________________________________________\n","gru (GRU)                    (64, None, 1024)          3938304   \n","_________________________________________________________________\n","dense (Dense)                (64, None, 65)            66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"thUszU9H7l6m"},"source":["TO get the actual prediction from the model you need to sample from the output distrubution to get actual character indices. This distribution is defined bt the logits over the character vocabulary.\n","\n","**NOTE**: It is important to sample from the distribution as taking the argmax of the distribution can easily get the model stuck in a loop."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fi_IwVp657n1","executionInfo":{"status":"ok","timestamp":1605796776140,"user_tz":-330,"elapsed":1533,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"fd1672fb-5370-4ab3-a02e-f3e1049e1f39"},"source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples = 1)\n","sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()\n","sampled_indices"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([63, 64, 48, 54, 49, 48, 29, 64, 42,  6, 35, 47, 27, 55,  3, 28, 33,\n","       17, 36, 17, 41, 54, 10, 34, 12, 44, 47,  2, 16, 49, 53, 63, 28, 58,\n","        6, 60, 48, 38, 57, 45, 29,  2, 14, 64, 35,  9,  8, 16, 39, 12, 49,\n","       52,  3, 61, 18, 21,  8, 52, 39,  9, 28, 45, 53, 53,  7, 47, 30, 46,\n","       52, 24, 54,  4, 62,  7, 21, 40, 31, 39, 18, 46,  4, 12,  7, 48, 61,\n","       15, 63, 10, 40, 40, 43, 37, 40, 27, 20, 22, 11, 48, 46, 26])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"6FG_U5mY8sDD"},"source":["This above output, gives us at each timestamp, a prediction of the next character index.\n","Decoding them further ..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15FXFxXo8j9u","executionInfo":{"status":"ok","timestamp":1605796955904,"user_tz":-330,"elapsed":1170,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"fbc6cf0e-aa0c-45d4-c677-15497138d5f0"},"source":["print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n","print()\n","print(\"Next char prediction: \\n\", repr(\"\".join(idx2char[sampled_indices])))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Input: \n"," 'nt to disinherit him,\\nWhich argued thee a most unloving father.\\nUnreasonable creatures feed their yo'\n","\n","Next char prediction: \n"," 'yzjpkjQzd,WiOq$PUEXEcp:V?fi!DkoyPt,vjZsgQ!BzW3.Da?kn$wFI.na3Pgoo-iRhnLp&x-IbSaFh&?-jwCy:bbeYbOHJ;jhN'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c5As2lFg9Xdn"},"source":["At this point the model is not at all trained, so we need to first train the model\n","\n","## Train the model\n","\n","### Attach an optimizer, and loss function:\n","\n","The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimensions of the predictions.\n","\n","because your model return logits, you need to set the 'from_logits' flag."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ibTfcXxd9QIl","executionInfo":{"status":"ok","timestamp":1605797540919,"user_tz":-330,"elapsed":1214,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"004d8595-466e-4666-9de3-acc7b1343ffc"},"source":["def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True )\n","\n","example_batch_loss = loss(target_example_batch, example_batch_predictions)\n","print(\"prediction shape: \",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","print(\"scalar_loss:      \",example_batch_loss.numpy().mean())"],"execution_count":24,"outputs":[{"output_type":"stream","text":["prediction shape:  (64, 100, 65) # (batch_size, sequence_length, vocab_size)\n","scalar_loss:       4.173478\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0M5QVX7t_icL"},"source":["## Compiling the model:"]},{"cell_type":"code","metadata":{"id":"PjUviltJ_cAU","executionInfo":{"status":"ok","timestamp":1605797577894,"user_tz":-330,"elapsed":1472,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["model.compile(optimizer='adam', loss = loss)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jlfI-fP1_r67"},"source":["**configure the checkpoints**\n","\n","use `tf.keras.callbacks.ModelCheckpoint`"]},{"cell_type":"code","metadata":{"id":"mQ21Ui0U_n6n","executionInfo":{"status":"ok","timestamp":1605797797544,"user_tz":-330,"elapsed":1629,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["# directory where the checkpoint will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoints files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath = checkpoint_prefix,\n","    save_weights_only = True\n",")"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"4rJskSoZAZS5","executionInfo":{"status":"ok","timestamp":1605797815200,"user_tz":-330,"elapsed":1600,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["EPOCHS = 10"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4qVS64OXAh0f","executionInfo":{"status":"ok","timestamp":1605797928860,"user_tz":-330,"elapsed":80646,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"717bb574-9b80-41cf-d83f-80989645071a"},"source":["history = model.fit(dataset ,epochs = EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","172/172 [==============================] - 7s 42ms/step - loss: 2.6909\n","Epoch 2/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.9626\n","Epoch 3/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.6937\n","Epoch 4/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.5433\n","Epoch 5/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.4541\n","Epoch 6/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.3929\n","Epoch 7/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.3475\n","Epoch 8/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.3084\n","Epoch 9/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.2733\n","Epoch 10/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.2395\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zx1t4-5kAofa","executionInfo":{"status":"ok","timestamp":1605798037943,"user_tz":-330,"elapsed":79694,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"79fb1aab-851e-4775-fdf3-1a749ba80512"},"source":["history = model.fit(dataset ,epochs = EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.2067\n","Epoch 2/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.1753\n","Epoch 3/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.1424\n","Epoch 4/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.1081\n","Epoch 5/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.0739\n","Epoch 6/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.0362\n","Epoch 7/10\n","172/172 [==============================] - 7s 42ms/step - loss: 1.0009\n","Epoch 8/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.9655\n","Epoch 9/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.9308\n","Epoch 10/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.8969\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eb02tn3SBFIr","executionInfo":{"status":"ok","timestamp":1605798125474,"user_tz":-330,"elapsed":80103,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"41635a11-0f5e-4a78-f865-ecb07b4d5e3c"},"source":["history = model.fit(dataset ,epochs = EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.8652\n","Epoch 2/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.8356\n","Epoch 3/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.8073\n","Epoch 4/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.7856\n","Epoch 5/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.7638\n","Epoch 6/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.7472\n","Epoch 7/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.7296\n","Epoch 8/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.7172\n","Epoch 9/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.7037\n","Epoch 10/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6949\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ijJ43VBnBaZ5","executionInfo":{"status":"ok","timestamp":1605798213366,"user_tz":-330,"elapsed":80487,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"1f975cae-f1ef-4764-818e-03add3a40199"},"source":["history = model.fit(dataset ,epochs = EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6850\n","Epoch 2/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6774\n","Epoch 3/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6693\n","Epoch 4/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6640\n","Epoch 5/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6581\n","Epoch 6/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6563\n","Epoch 7/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6502\n","Epoch 8/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6460\n","Epoch 9/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6444\n","Epoch 10/10\n","172/172 [==============================] - 7s 42ms/step - loss: 0.6439\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S4xE6qtrCLw4"},"source":["## Generate the Text"]},{"cell_type":"markdown","metadata":{"id":"fnqtrheECQbF"},"source":["To keep this prediction step simple, use a batch size of 1.\n","\n","Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n","\n","To run the model with a different batch_size, you need to rebuild the model and restore the weights from the checkpoint."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"hyRq8WOhBvxQ","executionInfo":{"status":"ok","timestamp":1605804434449,"user_tz":-330,"elapsed":1513,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"9fe3a4a3-803b-47c7-b9a9-d9f4f7f665f8"},"source":["tf.train.latest_checkpoint(checkpoint_dir)"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'./training_checkpoints/ckpt_10'"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"iag1OF0ICXtE","executionInfo":{"status":"ok","timestamp":1605804437323,"user_tz":-330,"elapsed":1684,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5pyHs2UCvVi","executionInfo":{"status":"ok","timestamp":1605804439250,"user_tz":-330,"elapsed":1007,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"6401d76d-e7ca-48b9-bed3-c3cd188eac7a"},"source":["model.summary()"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (1, None, 256)            16640     \n","_________________________________________________________________\n","gru_3 (GRU)                  (1, None, 1024)           3938304   \n","_________________________________________________________________\n","dense_3 (Dense)              (1, None, 65)             66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XfSUZXCBCxZ7","executionInfo":{"status":"ok","timestamp":1605804484118,"user_tz":-330,"elapsed":1532,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["def generate_text(model, start_string):\n","  # Evaluation step (generating text using the learned model)\n","\n","  # Number of characters to generate\n","  num_generate = 1000\n","\n","  # converting out start string to numbers (vectorizing)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # empty string to store our results\n","  text_generated = []\n","\n","  # Low temprature results in more predictable text\n","  # High temprature resuls in more surprising text\n","  # Experiment to find the best setting.\n","  temprature = 1.0\n","\n","  # Here batch_size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","    predictions = model(input_eval)\n","    #remove the batch dimension\n","    predictions  = tf.squeeze(predictions, 0)\n","\n","    # using a categorical distribution to predict the character returned by the model\n","    predictions = predictions/temprature\n","    predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n","\n","    #Pass the predicted character as the next input to the model\n","    # along with the previous hidden state\n","    input_eval = tf.expand_dims([predicted_id], 0)\n","\n","    text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string+ ''.join(text_generated))"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ioHaM2nRZbAi","executionInfo":{"status":"ok","timestamp":1605804490222,"user_tz":-330,"elapsed":5802,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"5e025e43-fb4b-430c-9a04-a6b283d7979f"},"source":["print(generate_text(model, start_string=u\"ROMEO: \"))"],"execution_count":65,"outputs":[{"output_type":"stream","text":["ROMEO: I dare not: but I for it was a thousand furlood of the child, why dost thou say'st he topsays here?\n","\n","ANGELO:\n","I was; whilst, as I say?\n","I charge you, cenvein to the duke;\n","Of Warwick are in readiness, mine eyes within.\n","\n","JULIET:\n","I will forget, nor any ot thy father's head.\n","\n","NORTHUMBERLAND:\n","Reed their choice:\n","Your honour natures me most fortunate though\n","That Edward father, this your companyour lady hath,\n","For then you know not what will hencenarr-look upon you: thou\n","I had eaten spotes me for this night\n","To see the slander of Gloucester, and how he doth attach and babe:\n","There lies to learn the ot of far absent:\n","'Tis well, some two days since, were worthy ofference\n","Nor do I'll fell this propos awhite:\n","Thou hadst choice in hope: he told me, did; there attact I did free thee.\n","\n","AUTOLYCUS:\n","Ha, ha!\n","This lamy and like to live with such a treech,\n","as enemies; we thou hadst, out of this prest\n","And say 'thar no man never come?\n","For this was ever man so much with sighs that joy\n","And in their hearts were in i\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hd9F014RdaIO"},"source":["## Advanced Customs:"]},{"cell_type":"markdown","metadata":{"id":"hoIMmAYldkg9"},"source":["## Advanced: Customized Training\n","\n","The above training procedure is simple, but does not give you much control.\n","\n","So now that you've seen how to run the model manually let's unpack the training loop, and implement it ourselves. This gives a starting point if, for example, you want to implement _curriculum learning_ to help stabilize the model's open-loop output.\n","\n","Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n","\n","The procedure works as follows:\n","\n","* First, reset the RNN state. You do this by calling the `tf.keras.Model.reset_states` method.\n","\n","* Next, iterate over the dataset (batch by batch) and calculate the *predictions* associated with each.\n","\n","* Open a `tf.GradientTape`, and calculate the predictions and loss in that context.\n","\n","* Calculate the gradients of the loss with respect to the model variables using the `tf.GradientTape.grads` method.\n","\n","* Finally, take a step downwards by using the optimizer's `tf.train.Optimizer.apply_gradients` method.\n"]},{"cell_type":"code","metadata":{"id":"pdSF2UhOZ7QM","executionInfo":{"status":"ok","timestamp":1605805507213,"user_tz":-330,"elapsed":2465,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["model = build_model(\n","    vocab_size,\n","    embedding_dim,\n","    rnn_units,\n","    BATCH_SIZE\n",")"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfvImfurd3ia","executionInfo":{"status":"ok","timestamp":1605805526723,"user_tz":-330,"elapsed":2930,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["optimizer = tf.keras.optimizers.Adam()"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIql0weDd8IO","executionInfo":{"status":"ok","timestamp":1605806575987,"user_tz":-330,"elapsed":1574,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}}},"source":["@tf.function\n","def train_step(inp, target):\n","  with tf.GradientTape() as tape:\n","    predictions = model(inp)\n","    loss = tf.reduce_mean(\n","        tf.keras.losses.sparse_categorical_crossentropy(target, predictions, from_logits=True)\n","    )\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","  return loss"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v72EjQZkekf1","executionInfo":{"status":"ok","timestamp":1605806655120,"user_tz":-330,"elapsed":77793,"user":{"displayName":"Gargeya Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhfCOHsQf3wIQd_JpZO8eMw9RObwoKP6_JeenN3yw=s64","userId":"17002557983865091801"}},"outputId":"e6792953-53ea-46d7-92e0-87d9b9cd91da"},"source":["# Training step \n","\n","EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  # resetting the hidden state at the start of every epoch\n","  model.reset_states()\n","\n","  for (batch_n, (inp, target)) in enumerate(dataset):\n","    loss = train_step(inp, target)\n","\n","    if batch_n% 100 == 0:\n","      template = \"Epoch {} Batch {} Loss {}\"\n","      print(template.format(epoch+1, batch_n, loss))\n","\n","  #   SAVING (checkpoint) the model every 5 epochs\n","  if (epoch+1)%5 == 0:\n","    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n","  print('Time taken for 1 epoch {} sec \\n'.format(time.time() - start))\n","\n","model.save_weights(checkpoint_prefix.format(epoch = epoch)) "],"execution_count":71,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n","Epoch 1 Batch 0 Loss 4.174774169921875\n","Epoch 1 Batch 100 Loss 2.3391036987304688\n","Epoch 1 Loss 2.1404\n","Time taken for 1 epoch 8.661991119384766 sec \n","\n","Epoch 2 Batch 0 Loss 2.1284615993499756\n","Epoch 2 Batch 100 Loss 1.9192508459091187\n","Epoch 2 Loss 1.7835\n","Time taken for 1 epoch 7.47285008430481 sec \n","\n","Epoch 3 Batch 0 Loss 1.7496970891952515\n","Epoch 3 Batch 100 Loss 1.6624188423156738\n","Epoch 3 Loss 1.6038\n","Time taken for 1 epoch 7.5009870529174805 sec \n","\n","Epoch 4 Batch 0 Loss 1.5730340480804443\n","Epoch 4 Batch 100 Loss 1.5703729391098022\n","Epoch 4 Loss 1.4782\n","Time taken for 1 epoch 7.445864915847778 sec \n","\n","Epoch 5 Batch 0 Loss 1.4458417892456055\n","Epoch 5 Batch 100 Loss 1.3901872634887695\n","Epoch 5 Loss 1.4395\n","Time taken for 1 epoch 7.483918190002441 sec \n","\n","Epoch 6 Batch 0 Loss 1.3959174156188965\n","Epoch 6 Batch 100 Loss 1.4176056385040283\n","Epoch 6 Loss 1.3659\n","Time taken for 1 epoch 7.545116186141968 sec \n","\n","Epoch 7 Batch 0 Loss 1.3281382322311401\n","Epoch 7 Batch 100 Loss 1.326464056968689\n","Epoch 7 Loss 1.3464\n","Time taken for 1 epoch 7.447384357452393 sec \n","\n","Epoch 8 Batch 0 Loss 1.2693532705307007\n","Epoch 8 Batch 100 Loss 1.303857684135437\n","Epoch 8 Loss 1.3060\n","Time taken for 1 epoch 7.459053039550781 sec \n","\n","Epoch 9 Batch 0 Loss 1.1927516460418701\n","Epoch 9 Batch 100 Loss 1.3093371391296387\n","Epoch 9 Loss 1.3032\n","Time taken for 1 epoch 7.630453824996948 sec \n","\n","Epoch 10 Batch 0 Loss 1.1998717784881592\n","Epoch 10 Batch 100 Loss 1.249182105064392\n","Epoch 10 Loss 1.2557\n","Time taken for 1 epoch 7.541879415512085 sec \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g8b5S5iahx2w"},"source":[""],"execution_count":null,"outputs":[]}]}